#!/usr/bin/env python3

from argostrain.dataset import *
from argostrain.sbd import *
from argostrain.multilang import *
import argostrain.xml

import random
from pathlib import Path

DEBUG = False
if 'DEBUG' in os.environ:
    DEBUG = os.environ['DEBUG'] in ['1', 'TRUE', 'True', 'true']

# Load data
if DEBUG:
    testdata_source = Path('data') / 'testdata_source'
    testdata_target = Path('data') / 'testdata_target'
    input_dataset = FileDataset(open(testdata_source), open(testdata_target))
else:
    available_datasets = get_available_datasets()
    # Get Spanish data
    es_data = list(filter(
            lambda x: x.to_code == 'es',
            available_datasets))
    input_dataset = es_data[0]

input_data_length = len(input_dataset)

# The dataset to use for training
dataset = CompositeDataset(copy_dataset(input_dataset))

# Generate SBD data
"""
sbd_length = int(input_data_length * 0.1)
sbd_dataset = generate_sbd_data(TrimmedDataset(input_dataset, sbd_length))
# At most use 0.1 as much sbd data as input data
trimmed_sbd_data = sbd_dataset.data(sbd_length)
sbd_dataset = Dataset(trimmed_sbd_data[0], trimmed_sbd_data[1])
"""

# Generate multi-lang
"""
langs_data = [(input_dataset, 'es'), (InvertedDataset(input_dataset), 'en')] 
multilang_dataset = generate_multilang_data(langs_data)
dataset += CompositeDataset(multilang_dataset)
"""

# Generate tag data
source, target = input_dataset.data()
tag_source = deque()
tag_target = deque()
for i in range(len(source)):
    normalized_tags = argostrain.xml.normalize_tags(source[i], target[i])
    if normalized_tags == None:
        continue
    tag_source.append(normalized_tags[0])
    tag_target.append(normalized_tags[1])
    info(f'Generating normalized tags {normalized_tags}')
tag_dataset = Dataset(tag_source, tag_target)
dataset += tag_dataset

# Generate capitalization data
CAPITAL_RATIO = 0.01
input_copy = copy_dataset(dataset)
capital_length = int(CAPITAL_RATIO * len(input_copy))
all_upper_dataset = TrimmedDataset(input_copy, capital_length)
all_upper_dataset = TransformedDataset(all_upper_dataset, str.upper)
all_lower_dataset = TrimmedDataset(input_copy, capital_length)
all_lower_dataset = TransformedDataset(all_lower_dataset, str.lower)
def random_caps(s):
    to_return = ''
    for c in s:
        caps = random.choice(range(3))
        if caps == 0:
            to_return += c.lower()
        elif caps == 1:
            to_return += c
        else:
            to_return += c.upper()
    return to_return
random_caps_dataset = TrimmedDataset(input_copy, capital_length)
random_caps_dataset = TransformedDataset(random_caps_dataset, random_caps)
all_caps_dataset = CompositeDataset(all_upper_dataset) + CompositeDataset(all_lower_dataset) + \
        CompositeDataset(random_caps_dataset)
dataset += all_caps_dataset

if DEBUG:
    print(dataset)
else:
    export_dataset(dataset)
